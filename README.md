# PySpark S3 Integration Example

This repository demonstrates how to use PySpark to interact with data stored in Amazon S3. It includes reading a CSV file from an S3 bucket, applying a filter on the data, and writing the filtered output back to S3.

## Prerequisites

Before running this code, ensure you have the following:

- **Apache Spark** (with PySpark installed) on your system -version 3.5.2
- **Java** (for Spark execution) -version-11.
- **AWS Account** with access to S3.
- **AWS credentials** (Access Key ID and Secret Access Key).

## Setup

### 1. Install Dependencies

You need to have PySpark installed. You can install it via `pip`:

```bash
pip install pyspark
```
